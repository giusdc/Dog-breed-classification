{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import random \n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n",
    "from tensorflow.keras.applications.nasnet import NASNetLarge\n",
    "\n",
    "#Def variables\n",
    "pathTraining= '../input/dataset/Dataset/train'\n",
    "images= []\n",
    "labels = []\n",
    "breeds=[]\n",
    "images_val=[]\n",
    "labels_val=[]\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=150\n",
    "INITIALRATE=0.01\n",
    "random.seed(456)\n",
    "seed=456\n",
    "random.random()\n",
    "\n",
    "#Preprocessing function\n",
    "def preprocess(img_path, label):\n",
    "    img_raw = tf.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(img_raw, channels=3)\n",
    "    image = tf.image.resize_images(image, [331, 331])\n",
    "    image /= 255.0  # normalize to [0,1] range\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    label = tf.one_hot(label,35)\n",
    "    return image,label\n",
    "\n",
    "#Data augmentation function\n",
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image, seed=seed)\n",
    "    image = tf.image.random_saturation(image,0.8,1.8,seed=seed)\n",
    "    image = tf.image.random_brightness(image, 0.08,seed=seed)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.5,seed=seed)\n",
    "    return image, label\n",
    "\n",
    "#Get all dataset values\n",
    "def getDataset():\n",
    "    x=0\n",
    "    for r, d, f in os.walk(pathTraining):\n",
    "        for directory in d:\n",
    "            #Get path of the dir\n",
    "            path=pathTraining+\"/\"+directory\n",
    "            #Append the breed name\n",
    "            breeds.append(directory)\n",
    "            for r,d,f in os.walk(path):\n",
    "                for file in f:\n",
    "                    images.append(os.path.join(r, file))\n",
    "                    labels.append(x)\n",
    "            x+=1\n",
    "    return images,labels\n",
    "\n",
    "#Learning rate decay function \n",
    "def step_decay(epoch):\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = INITIALRATE * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "\n",
    "#Save the model with the highest accuracy and lowest loss\n",
    "best_val_acc = 0\n",
    "best_val_loss = sys.float_info.max \n",
    "def saveModel(epoch,logs):\n",
    "    global best_val_acc\n",
    "    global best_val_loss\n",
    "    val_acc = logs['val_acc']\n",
    "    val_loss = logs['val_loss']\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        model.save(\"big.h5\")\n",
    "        print('Save model')\n",
    "    elif val_acc == best_val_acc:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss=val_loss\n",
    "            model.save(\"big.h5\")\n",
    "            print('Save model with low ')\n",
    "#Split the dataset in train and validation set\n",
    "images,labels=getDataset()\n",
    "imagesTrain, imagesVal, labelsTrain, labelsVal = train_test_split(images, labels, test_size=0.20)\n",
    "datasetTrain = tf.data.Dataset.from_tensor_slices((imagesTrain, labelsTrain))\n",
    "datasetVal = tf.data.Dataset.from_tensor_slices((imagesVal, labelsVal))\n",
    "trainSize = len(imagesTrain)\n",
    "valSize=len(imagesVal)\n",
    "#Shuffle datasets\n",
    "datasetTrain.shuffle(len(imagesTrain),seed=seed)\n",
    "datasetVal.shuffle(len(imagesVal),seed=seed)\n",
    "#Apply preprocessing and augmentation functions(only for the training set)\n",
    "datasetTrain = datasetTrain.map(preprocess)\n",
    "datasetTrain = datasetTrain.map(augment)\n",
    "datasetVal = datasetVal.map(preprocess)\n",
    "#Set batch size parameter and epochs\n",
    "datasetTrain = datasetTrain.batch(BATCH_SIZE, drop_remainder=True)\n",
    "datasetTrain = datasetTrain.repeat(EPOCHS)\n",
    "datasetVal = datasetVal.batch(BATCH_SIZE, drop_remainder=True)\n",
    "datasetVal = datasetVal.repeat(EPOCHS)\n",
    "#Inizialize the iterator\n",
    "iterator_train = datasetTrain.make_initializable_iterator()\n",
    "iterator_val=  datasetVal.make_initializable_iterator()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Define the architecture\n",
    "convBase =tf.keras.applications.nasnet.NASNetLarge(input_shape=None, include_top=False, weights='imagenet', input_tensor=None, pooling=None , classes=1000)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(convBase)\n",
    "#Set the convBase layer not trainable\n",
    "model.layers[0].trainable=False\n",
    "model.add(tf.keras.layers.GlobalMaxPooling2D())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(35,kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.GaussianNoise(0.5))\n",
    "model.add(tf.keras.layers.Activation('softmax'))\n",
    "#Compile the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adagrad(lr=INITIALRATE, epsilon=None, decay=0.0),metrics=['accuracy'])\n",
    "#Plot the model architecture in a file\n",
    "tf.keras.utils.plot_model(model, to_file='model.png',show_shapes=True)\n",
    "tf.keras.backend.get_session().run(iterator_train.initializer)\n",
    "tf.keras.backend.get_session().run(iterator_val.initializer)\n",
    "#Train model\n",
    "model.fit(iterator_train,validation_data=iterator_val,steps_per_epoch=math.floor(trainSize/BATCH_SIZE),validation_steps=math.floor(valSize/BATCH_SIZE),callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True),tf.keras.callbacks.LearningRateScheduler(step_decay),tf.keras.callbacks.LambdaCallback(on_epoch_end=saveModel)],epochs=EPOCHS)\n",
    "#Load the best model\n",
    "del model\n",
    "model = tf.keras.models.load_model('big.h5')\n",
    "#Evaluate the model on validation set\n",
    "model.evaluate(iterator_val, batch_size=BATCH_SIZE, verbose=1, sample_weight=None, steps=math.floor(valSize/BATCH_SIZE), callbacks=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create the file for the submission\n",
    "imagesT = []\n",
    "labelsT = []\n",
    "pathTest='../input/dataset/Dataset/test'\n",
    "x = 0;\n",
    "#We append a fake label only for creating the iterator,\n",
    "#but this label doesn't be considerated in the prediction of the class\n",
    "for r, d, f in os.walk(pathTest):\n",
    "    for file in f:\n",
    "        imagesT.append(os.path.join(r, file))\n",
    "        labelsT.append(x)           \n",
    "    x+=1\n",
    "datasetTest = tf.data.Dataset.from_tensor_slices((imagesT, labelsT))\n",
    "datasetTest = datasetTest.map(preprocess)\n",
    "datasetTest = datasetTest.batch(1, drop_remainder=True)\n",
    "datasetTest = datasetTest.repeat(1)\n",
    "iteratorTest = datasetTest.make_initializable_iterator()\n",
    "tf.keras.backend.get_session().run(iteratorTest.initializer)\n",
    "testSize=len(imagesT)\n",
    "#Predict the classes\n",
    "predictionClasses = model.predict(iteratorTest, steps=testSize)\n",
    "classes=[]\n",
    "fileIds=[]\n",
    "#Get the file id\n",
    "for i in range(testSize):\n",
    "    image = imagesT[i]\n",
    "    imageList = image.split(\"/\")\n",
    "    imageId = (imageList[-1])[:-4]\n",
    "    fileIds.append(imageId)\n",
    "#Get the predicted breed\n",
    "for j in range(len(predictionClasses)):\n",
    "    classes.append(breeds[np.argmax(predictionClasses[j])])\n",
    "\n",
    "#Submission\n",
    "submission = pd.DataFrame({'id':fileIds,'breed':classes})\n",
    "submission.head()\n",
    "filename = 'result.csv'\n",
    "submission.to_csv(filename,index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-8d75aaba",
   "language": "python",
   "display_name": "PyCharm (Fork of Ultimissimo.ipynb)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}